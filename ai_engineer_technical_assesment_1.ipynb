{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Engineer Technical Assessment\n",
    "\n",
    "## Overview\n",
    "Build an AI-powered solution for sentiment analysis of movie reviews that leverages the existing dataset to improve accuracy. This assessment is designed to be completed in 2-3 hours, we do NOT expect very detailed answers or long explanations.\n",
    "\n",
    "## Notes\n",
    "- AI assistance is allowed and, in fact, encouraged. caveats are:\n",
    "    - Concise explanations and simple code are preferred\n",
    "    - Solutions that use newer information and go beyond LLMs cuttof date are valuable.\n",
    "    - You must be able to explain the code you write here\n",
    "\n",
    "- Look up any information you need, copy and paste code is allowed.\n",
    "- Setup the environment as needed. You can use your local environment, colab, or any other environment of your preferenc.\n",
    "- Focus on working solutions, leave iteration and improvements if you have extra time.\n",
    "\n",
    "## Setup\n",
    "The following cells will download and prepare the IMDB dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 5000\n",
      "Test samples: 10\n",
      "\n",
      "Sample review:\n",
      "Text: Dumb is as dumb does, in this thoroughly uninteresting, supposed black comedy. Essentially what starts out as Chris Klein trying to maintain a low profile, eventually morphs into an uninspired version...\n",
      "Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load IMDB dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "\n",
    "# Sample subset for quicker development\n",
    "train_df = train_df.sample(n=5000, random_state=42)\n",
    "test_df = test_df.sample(n=10, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample review:\")\n",
    "sample = train_df.iloc[0]\n",
    "print(f\"Text: {sample['text'][:200]}...\")\n",
    "print(f\"Sentiment: {'Positive' if sample['label'] == 1 else 'Negative'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6868</th>\n",
       "      <td>Dumb is as dumb does, in this thoroughly unint...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24016</th>\n",
       "      <td>I dug out from my garage some old musicals and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9668</th>\n",
       "      <td>After watching this movie I was honestly disap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13640</th>\n",
       "      <td>This movie was nominated for best picture but ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14018</th>\n",
       "      <td>Just like Al Gore shook us up with his painful...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "6868   Dumb is as dumb does, in this thoroughly unint...      0\n",
       "24016  I dug out from my garage some old musicals and...      1\n",
       "9668   After watching this movie I was honestly disap...      0\n",
       "13640  This movie was nominated for best picture but ...      1\n",
       "14018  Just like Al Gore shook us up with his painful...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Model Implementation\n",
    "Implement a solution that analyzes sentiment in movie reviews. This part is explicitly open-ended: Explore ways to leverage the example dataset to enhance predictions. You can consider a pre-trained language model that can understand and generate text, external API's, RAG systems etc. \n",
    "Feel free to use any library or tool you are comfortable with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address this sentiment analysis assessment, I have chosen to evaluate two approaches:\n",
    "\n",
    "1. **Lightweight Encoder Model (ModernBERT):** Using an encoder-only transformer model trained for sentiment classification. Specifically, I have decided to use [ModernBERT](https://arxiv.org/pdf/2412.13663), a next-generation encoder model (2024) that introduces several architectural advancements over the original BERT.\n",
    "2. **Sentiment Classification with a Small LLM:** The second approach explores the use of a small-scale Large Language Model (LLM), ranging between 1B to 3B parameters, to perform sentiment classification through prompt engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightweight Encoder Model (ModernBERT)\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "class LightweightModelService:\n",
    "    def __init__(self, model_name: str =\"clapAI/modernBERT-base-multilingual-sentiment\") -> None:\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Load the tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, torch_dtype=torch.float16).to(self.device).eval()\n",
    "\n",
    "    def predict(self, movie_review: str) -> tuple[str, float]:\n",
    "        inputs = self.tokenizer(movie_review, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = self.model(**inputs)\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            confidence, prediction = torch.max(probs, dim=-1)\n",
    "\n",
    "        label_map = self.model.config.id2label\n",
    "        sentiment = label_map[prediction.item()].lower()  \n",
    "\n",
    "        # Map neutral to negative for binary compatibility\n",
    "        if sentiment == \"neutral\":\n",
    "            sentiment = \"negative\"\n",
    "\n",
    "        return sentiment, round(confidence.item(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Any, cast\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline  # type: ignore\n",
    "\n",
    "### Instructions:\n",
    "# System-level prompt defining the assistant's behavior\n",
    "def get_system_prompt() -> str:\n",
    "    return \"\"\"You are an expert in sentiment analysis of movie reviews. Your task is to evaluate the sentiment of the given review and classify it as \"positive\" or \"negative\", and provide a confidence between 0-1. Your analysis must consider both explicit and implicit sentiment cues while maintaining a focus on the target entity. \n",
    "    \n",
    "    1. **Sentiment Classification Criteria**:  \n",
    "    - **Positive**: The review expresses favorable opinions about the movie, highlighting its strengths or praising specific aspects.  \n",
    "    - **Negative**: The review conveys unfavorable opinions, criticizing elements of the movie or expressing disappointment.\n",
    "\n",
    "    2. **Handling Mixed Sentiments and Implicit Sentiment**:  \n",
    "    - If both positive and negative elements exist, classify based on the **dominant sentiment**, considering intensity and frequency.  \n",
    "    - Detect subtle tones, including **irony, implied sentiment, and framing biases** (e.g., selective comparisons, loaded phrases). \n",
    "\n",
    "    3. **Classification Constraints**:\n",
    "        - Never return \"neutral\". If the sentiment is unclear or confidence is low, default to \"negative\". \n",
    "\n",
    "    4. **Output Format**:  \n",
    "    - Return:\n",
    "        classification: <positive|negative>  \n",
    "        confidence: <float between 0 and 1>\n",
    "    - Do not include explanations, additional text, or punctuation.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def get_system_prompt_with_examples(examples) -> str:\n",
    "    \n",
    "    base_prompt = \"\"\"You are an expert in sentiment analysis of movie reviews. Your task is to evaluate the sentiment of the given review and classify it as \"positive\" or \"negative\", and provide a confidence between 0-1. Your analysis must consider both explicit and implicit sentiment cues while maintaining a focus on the target entity. \n",
    "    \n",
    "    1. **Sentiment Classification Criteria**:  \n",
    "    - **Positive**: The review expresses favorable opinions about the movie, highlighting its strengths or praising specific aspects.  \n",
    "    - **Negative**: The review conveys unfavorable opinions, criticizing elements of the movie or expressing disappointment.\n",
    "\n",
    "    2. **Handling Mixed Sentiments and Implicit Sentiment**:  \n",
    "    - If both positive and negative elements exist, classify based on the **dominant sentiment**, considering intensity and frequency.\n",
    "    - Detect subtle tones, including **irony, implied sentiment, and framing biases** (e.g., selective comparisons, loaded phrases).  \n",
    "\n",
    "    3. **Classification Constraints**:\n",
    "        - Never return \"neutral\". If the sentiment is unclear or confidence is low, default to \"negative\".\n",
    "\n",
    "    4. **Output Format**:  \n",
    "    - Return:\n",
    "        classification: <positive|negative>  \n",
    "        confidence: <float between 0 and 1>\n",
    "    - Do not include explanations, additional text, or punctuation.\n",
    "\n",
    "    ## Examples\n",
    "    \"\"\"\n",
    "    formatted_examples = \"\"\n",
    "    for ex in examples:\n",
    "        formatted_examples += (\n",
    "            f\"Review: {ex['text']}\\n\"\n",
    "            f\"classification: {ex['label']}\\n\"\n",
    "        )\n",
    "\n",
    "    return base_prompt + \"\\n\" + formatted_examples.strip()\n",
    "\n",
    "\n",
    "# User-level prompt defining the interaction model\n",
    "def get_user_prompt(movie_review: str) -> str:\n",
    "    return f\"\"\"Analyze the sentiment of the provided movie review: {movie_review}\"\"\"\n",
    "\n",
    "class Phi4LLM:\n",
    "    \"\"\"\n",
    "    A service class for interacting with the PHI 4 model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model_name: str = \"microsoft/Phi-4-mini-instruct\", temperature: float = 0.1, max_new_tokens: int = 500\n",
    "    ) -> None:\n",
    "        torch.random.manual_seed(0)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.temperature = temperature\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.initialize_model()\n",
    "\n",
    "    def initialize_model(self) -> None:\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            **self.get_model_kwargs(),\n",
    "        ).to(self.device).eval()\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.completion_pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.model,\n",
    "                tokenizer=self.tokenizer,\n",
    "            )\n",
    "\n",
    "    def get_completions(self, movie_review: str, examples: list[dict] = None) -> tuple[str, float]:\n",
    "        \"\"\"\n",
    "        Generate completions based on the provided messages.\n",
    "        \"\"\"\n",
    "        if examples is not None and not examples.empty:\n",
    "            few_shot_examples = [\n",
    "                {\n",
    "                    \"text\": row[\"text\"],\n",
    "                    \"label\": \"positive\" if row[\"label\"] == 1 else \"negative\",\n",
    "                }\n",
    "                for _, row in examples.iterrows()\n",
    "            ]\n",
    "            system_prompt = get_system_prompt_with_examples(few_shot_examples)\n",
    "        else:\n",
    "            system_prompt = get_system_prompt()\n",
    "            \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": get_user_prompt(movie_review),\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        generation_args = {\n",
    "            \"max_new_tokens\": self.max_new_tokens,\n",
    "            \"return_full_text\": False,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"do_sample\": True,\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = self.completion_pipeline(messages, **generation_args)\n",
    "\n",
    "        return self.parse_llm_output(cast(str, output[0][\"generated_text\"]))\n",
    "    \n",
    "    def parse_llm_output(self, llm_output: str) -> tuple[str, float]:\n",
    "        \"\"\"\n",
    "        Parse the LLM response to extract sentiment and confidence.\n",
    "        \"\"\"\n",
    "        classification_match = re.search(r\"classification:\\s*(positive|negative|neutral)\", llm_output, re.IGNORECASE)\n",
    "        confidence_match = re.search(r\"confidence:\\s*([0-1](?:\\.\\d+)?)\", llm_output)\n",
    "\n",
    "        if classification_match and confidence_match:\n",
    "            classification = classification_match.group(1).lower()\n",
    "            confidence = float(confidence_match.group(1))\n",
    "\n",
    "            # Map neutral to negative for binary compatibility\n",
    "            if classification == \"neutral\":\n",
    "                classification = \"negative\"\n",
    "            \n",
    "            return classification, confidence\n",
    "\n",
    "        raise ValueError(f\"Could not parse output: {llm_output}\")\n",
    "\n",
    "    def get_model_kwargs(self) -> dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Return default kwargs for huggingface model loading.\n",
    "        \"\"\"\n",
    "        model_kwargs = {\n",
    "            \"device_map\": self.device,\n",
    "            \"torch_dtype\": \"auto\",\n",
    "            \"trust_remote_code\": True,\n",
    "        }\n",
    "    \n",
    "        # if self.device.type == \"cuda\":\n",
    "        #     model_kwargs[\"attn_implementation\"] = \"flash_attention_2\"\n",
    "\n",
    "        return model_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: API Implementation\n",
    "Create a simple API using FastAPI that serves your solution. The API should accept a review text and return the sentiment analysis result.\n",
    "\n",
    "Expected format:\n",
    "```python\n",
    "# Request\n",
    "{\n",
    "    \"review_text\": \"This movie exceeded my expectations...\"\n",
    "}\n",
    "\n",
    "# Response\n",
    "{\n",
    "    \"sentiment\": \"positive\",\n",
    "    \"confidence\": 0.92,\n",
    "    \"similar_reviews\": [\n",
    "        {},\n",
    "        {}\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d718e422cd2445488d5fb77a3cb334e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from fastapi import FastAPI, HTTPException\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "# Your API implementation here\n",
    "class ClassificationInputRequest(BaseModel):\n",
    "    review_text: str\n",
    "\n",
    "class ClassificationResponse(BaseModel):\n",
    "    sentiment: str # Should be positive or negative\n",
    "    confidence: Optional[float] \n",
    "    similar_reviews: List[Dict]\n",
    "\n",
    "class HealthCheckResponse(BaseModel):\n",
    "    status: str\n",
    "    version: str\n",
    "    timestamp: str  \n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "lightweight_model = LightweightModelService()\n",
    "llm_model = Phi4LLM()\n",
    "\n",
    "@app.post(\"/sentiment-classification-lightweight\", response_model=ClassificationResponse)\n",
    "async def sentiment_predict_lightweight(request: ClassificationInputRequest):\n",
    "    try:\n",
    "        result = lightweight_model.predict(request.review_text)\n",
    "        sentiment, confidence = result\n",
    "\n",
    "        return ClassificationResponse(\n",
    "            sentiment=sentiment,\n",
    "            confidence=confidence,\n",
    "            similar_reviews=[]\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        raise HTTPException(detail=str(e), status_code=400)\n",
    "\n",
    "@app.post(\"/sentiment-classification-llm\", response_model=ClassificationResponse)\n",
    "async def sentiment_predict_llm(request: ClassificationInputRequest):\n",
    "    try:\n",
    "        result = llm_model.get_completions(request.review_text)\n",
    "        sentiment, confidence = result\n",
    "\n",
    "        return ClassificationResponse(\n",
    "            sentiment=sentiment,\n",
    "            confidence=confidence,\n",
    "            similar_reviews=[]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(detail=str(e), status_code=400)\n",
    "    \n",
    "@app.get(\"/health\", response_model=HealthCheckResponse)\n",
    "async def health_check():\n",
    "    return HealthCheckResponse(\n",
    "        status=\"healthy\",\n",
    "        version=\"1.0.0\",\n",
    "        timestamp=datetime.utcnow().isoformat()\n",
    "    )\n",
    "\n",
    "# Code for run a FastAPI server inside a Jupyter notebook\n",
    "\n",
    "# import uvicorn\n",
    "# import threading\n",
    "\n",
    "# def run_api():\n",
    "#     uvicorn.run(app, host=\"0.0.0.0\", port=8000, reload=False)\n",
    "\n",
    "# thread = threading.Thread(target=run_api, daemon=True)\n",
    "# thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Testing and Performance\n",
    "Evaluate your solution's performance on the test set. Include:\n",
    "1. Accuracy metrics (precision, recall, F1-score)\n",
    "2. Inference speed (average time per prediction)\n",
    "\n",
    "Compare performance with and without using the example data to demonstrate any improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Your testing code here\n",
    "def evaluate_model(model, model_type=\"lightweight\"):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    times = []\n",
    "\n",
    "    for review, label in zip(test_df[\"text\"], test_df[\"label\"]):\n",
    "        start = time.time()\n",
    "\n",
    "        if model_type == \"lightweight\":\n",
    "            pred_label, _ = model.predict(review)\n",
    "        elif model_type == \"llm_with_examples\":\n",
    "            few_shot_df = train_df.sample(n=3, random_state=123)\n",
    "            pred_label, _ = model.get_completions(review, few_shot_df)\n",
    "        else:\n",
    "            pred_label, _ = model.get_completions(review)\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        pred_bin = 1 if pred_label == \"positive\" else 0\n",
    "        y_true.append(label)\n",
    "        y_pred.append(pred_bin)\n",
    "        times.append(end - start)\n",
    "\n",
    "    print(f\"\\nEvaluation for {model_type} model\")\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"negative\", \"positive\"]))\n",
    "    print(f\"Avg inference time per review: {round(sum(times) / len(times), 4)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing a small set for the evaluation. To select the best model for make the complete evaluation. \n",
    "eval_test_df = pd.DataFrame(dataset['test'])\n",
    "test_df = eval_test_df.sample(n=50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation for lightweight model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.97      1.00      0.98        28\n",
      "    positive       1.00      0.95      0.98        22\n",
      "\n",
      "    accuracy                           0.98        50\n",
      "   macro avg       0.98      0.98      0.98        50\n",
      "weighted avg       0.98      0.98      0.98        50\n",
      "\n",
      "Avg inference time per review: 0.4188 seconds\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(lightweight_model, model_type=\"lightweight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation for llm model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      1.00      0.89        28\n",
      "    positive       1.00      0.68      0.81        22\n",
      "\n",
      "    accuracy                           0.86        50\n",
      "   macro avg       0.90      0.84      0.85        50\n",
      "weighted avg       0.89      0.86      0.85        50\n",
      "\n",
      "Avg inference time per review: 2.7652 seconds\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(llm_model, model_type=\"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation for llm_with_examples model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        28\n",
      "    positive       0.95      0.86      0.90        22\n",
      "\n",
      "    accuracy                           0.92        50\n",
      "   macro avg       0.93      0.91      0.92        50\n",
      "weighted avg       0.92      0.92      0.92        50\n",
      "\n",
      "Avg inference time per review: 6.2335 seconds\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(llm_model, model_type=\"llm_with_examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Evaluation Analysis\n",
    "Based on the results from the small evaluation set, the **best performing and fastest model** is the `lightweight_model` (ModernBERT). In my view, this is expected, as it is a task-specific model trained explicitly for sentiment classification.\n",
    "\n",
    "The LLM (`Phi-4-mini`), although not optimized for this task, still achieves good results. However, it is significantly slower and more resource-intensive, making it less suitable for production use in this context. \n",
    "\n",
    "We shoud take into account that the the few-shot prompting approach (LLM with examples) shows an improvement in accuracy and F1-score compared to the zero-shot version. This confirms that providing examples in the prompt helps the LLM better align with task expectations, although at the cost of increased inference time. \n",
    "\n",
    "Given these findings, I will now perform a full evaluation on the entire IMDB test set using only the `lightweight_model`, which is more appropriate for this use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation for lightweight model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.98      0.99      0.98     12500\n",
      "    positive       0.99      0.98      0.98     12500\n",
      "\n",
      "    accuracy                           0.98     25000\n",
      "   macro avg       0.98      0.98      0.98     25000\n",
      "weighted avg       0.98      0.98      0.98     25000\n",
      "\n",
      "Avg inference time per review: 0.0426 seconds\n"
     ]
    }
   ],
   "source": [
    "eval_complete_test_df = pd.DataFrame(dataset['test'])\n",
    "test_df = eval_complete_test_df.sample(n=25000, random_state=42)\n",
    "\n",
    "evaluate_model(lightweight_model, model_type=\"lightweight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Deployment Strategy\n",
    "\n",
    "1. Describe your deployment strategy considering:\n",
    "   - Data storage and retrieval\n",
    "   - Scalability\n",
    "   - Resource requirements\n",
    "   - Cost considerations\n",
    "\n",
    "2. Create a simple Dockerfile to package your solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Deployment Strategy\n",
      "\n",
      "## Infrastructure\n",
      "\n",
      "For a simple and portable setup, we will containerize the ModernBERT sentiment analysis API using Docker. The API itself will be built with FastAPI and uvicorn. The idea, is to package the model and the app in a docker container, which ensures consistency across environments and makes it easy to deploy on any cloud or platform.\n",
      "    - Deployment Target: We want to use docker as it can be deployed on a cloud service (AWS, Baseten, HuggingFace Inference Endpoints). A typical approach would involve AWS Elastic Container Service (ECS) or Google Cloud Run. These services can directly run the container without needing to manage servers, aligning with the goal of simplicity and portability. \n",
      "    - Load Balancer: In a production setup, the container would be placed behind a load balancer. For instance, if deploying on AWS, an Application Load Balancer (ALB) would handle HTTPS termination and route traffic to the FastAPI container. If self-hosting, an Nginx reverse proxy could be used to handle TLS and route traffic to Uvicorn.\n",
      "\n",
      "Overall, this infrastructure prioritizes simplicity and portability. Once the Docker image is built, deploying to AWS, GCP, or Render is as simple as pushing the image and letting the platform run it.\n",
      "\n",
      "## Scalability Approach\n",
      "\n",
      "To handle increasing traffic, the deployment will utilize horizontal scaling\n",
      "\n",
      "    - Horizontal Scaling with Load Balancing: As demand grows, we can run multiple instances of the Docker container across different nodes. A load balancer will distribute incoming requests to avoid overloading a single instance. For example, on AWS, this could be implemented via ECS tasks or EC2 instances behind an ALB; in Kubernetes, using a Service with autoscaled pods.\n",
      "    - Autoscaling Strategies: We could define autoscaling rules based on CPU or GPU usage (e.g., add a new container when usage exceeds 70%). Platforms like Cloud Run and Baseten offer scale-to-zero capabilities, reducing cost during idle times. Additionally, a scale-to-zero approach can be considered on platforms like Cloud Run or AWS Lambda – meaning if there's no traffic, instances scale down to zero to save cost, and scale up on incoming requests.\n",
      "\n",
      "Note: Platforms like Baseten or Hugging Face Endpoints also handle autoscaling internally, so if operational simplicity is preferred, these are viable alternatives.\n",
      "\n",
      "## Model & Data Storage\n",
      "\n",
      "The ModernBERT model should be accessible at runtime:\n",
      "\n",
      "    - Model Packaging: The model will be embedded directly in the Docker image using the Hugging Face `transformers` library. This ensures fast startup and avoids downloading weights during runtime.\n",
      "    - Data Storage and Logging: The API is stateless and does not persist input/output data. However, we could store some logs results if needed. In addition to that, we might want to store some data for monitoring or possible improvements.\n",
      "        - Logs: Log requests and responses (excluding sensitive data) for debugging or analytics.\n",
      "        - Caching Layer: If certain inputs are very frequent, a cache could be used to speed up repeated queries. Given this task is quite fast, caching is not critical unless we encounter specific hot inputs.\n",
      "\n",
      "## Resource & Cost Considerations\n",
      "\n",
      "One of the reasons to choose ModernBERT (a lightweight model) is its small size and fast inference. This allows us to deploy it on low-cost infrastructure. Therefore, probably we could use a CPU for cost effiencey.  A small instance (e.g., 2 vCPU) is sufficient for moderate traffic.\n",
      "\n",
      "\n",
      "Dockerfile:\n",
      "\n",
      "FROM python:3.10-slim\n",
      "\n",
      "# Set working directory\n",
      "WORKDIR /app\n",
      "\n",
      "# Install uv (Python package manager)\n",
      "RUN apt-get update && apt-get install -y curl && \\\n",
      "    curl -LsSf https://astral.sh/uv/install.sh | sh\n",
      "\n",
      "# Add uv to PATH\n",
      "ENV PATH=\"/root/.cargo/bin:$PATH\"\n",
      "\n",
      "# Copy project files and install dependencies\n",
      "COPY pyproject.toml .\n",
      "COPY uv.lock .  # Optional, if you have a lock file\n",
      "RUN uv sync\n",
      "\n",
      "# Copy the actual app code\n",
      "COPY . /app\n",
      "\n",
      "# Expose FastAPI port\n",
      "EXPOSE 8000\n",
      "\n",
      "# Start the API server\n",
      "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
      "\n",
      "Note: This Dockerfile assumes your FastAPI app is defined in a standalone Python file (e.g., `main.py`). If you're currently running everything in a Jupyter notebook, you'd need to extract the API logic into a file for deployment.\n"
     ]
    }
   ],
   "source": [
    "# Write your deployment strategy here as a markdown cell\n",
    "\n",
    "deployment_strategy = \"\"\"\n",
    "# Deployment Strategy\n",
    "\n",
    "## Infrastructure\n",
    "\n",
    "For a simple and portable setup, we will containerize the ModernBERT sentiment analysis API using Docker. The API itself will be built with FastAPI and uvicorn. The idea, is to package the model and the app in a docker container, which ensures consistency across environments and makes it easy to deploy on any cloud or platform.\n",
    "    - Deployment Target: We want to use docker as it can be deployed on a cloud service (AWS, Baseten, HuggingFace Inference Endpoints). A typical approach would involve AWS Elastic Container Service (ECS) or Google Cloud Run. These services can directly run the container without needing to manage servers, aligning with the goal of simplicity and portability. \n",
    "    - Load Balancer: In a production setup, the container would be placed behind a load balancer. For instance, if deploying on AWS, an Application Load Balancer (ALB) would handle HTTPS termination and route traffic to the FastAPI container. If self-hosting, an Nginx reverse proxy could be used to handle TLS and route traffic to Uvicorn.\n",
    "\n",
    "Overall, this infrastructure prioritizes simplicity and portability. Once the Docker image is built, deploying to AWS, GCP, or Render is as simple as pushing the image and letting the platform run it.\n",
    " \n",
    "## Scalability Approach\n",
    "\n",
    "To handle increasing traffic, the deployment will utilize horizontal scaling\n",
    "\n",
    "    - Horizontal Scaling with Load Balancing: As demand grows, we can run multiple instances of the Docker container across different nodes. A load balancer will distribute incoming requests to avoid overloading a single instance. For example, on AWS, this could be implemented via ECS tasks or EC2 instances behind an ALB; in Kubernetes, using a Service with autoscaled pods.\n",
    "    - Autoscaling Strategies: We could define autoscaling rules based on CPU or GPU usage (e.g., add a new container when usage exceeds 70%). Platforms like Cloud Run and Baseten offer scale-to-zero capabilities, reducing cost during idle times. Additionally, a scale-to-zero approach can be considered on platforms like Cloud Run or AWS Lambda – meaning if there's no traffic, instances scale down to zero to save cost, and scale up on incoming requests.\n",
    "\n",
    "Note: Platforms like Baseten or Hugging Face Endpoints also handle autoscaling internally, so if operational simplicity is preferred, these are viable alternatives.\n",
    "\n",
    "## Model & Data Storage\n",
    "\n",
    "The ModernBERT model should be accessible at runtime:\n",
    "\n",
    "    - Model Packaging: The model weights will be downloaded at build time and included in the Docker image using the Hugging Face `transformers` library. This ensures fast startup and avoids downloading weights during runtime.\n",
    "    - Data Storage and Logging: The API is stateless and does not persist input/output data. However, we could store some logs results if needed. Optionally, we can log request metadata for monitoring and future model refinement.\n",
    "        - Logs: Log requests and responses (excluding sensitive data) for debugging or analytics.\n",
    "        - Caching Layer: If certain inputs are very frequent, a cache could be used to speed up repeated queries. Given this task is quite fast, caching is not critical unless we encounter specific hot inputs.\n",
    "\n",
    "## Resource & Cost Considerations\n",
    "\n",
    "One of the reasons to choose ModernBERT (a lightweight model) is its small size and fast inference. This allows us to deploy it on low-cost infrastructure. Therefore, probably we could use a CPU for cost effiencey.  A small instance (e.g., 2 vCPU) is sufficient for moderate traffic.\n",
    "\"\"\"\n",
    "\n",
    "print(deployment_strategy)\n",
    "\n",
    "# Write your Dockerfile content\n",
    "dockerfile_content = \"\"\"\n",
    "FROM python:3.10-slim\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Install uv (Python package manager)\n",
    "RUN apt-get update && apt-get install -y curl && \\\\\n",
    "    curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Add uv to PATH\n",
    "ENV PATH=\"/root/.cargo/bin:$PATH\"\n",
    "\n",
    "# Copy project files and install dependencies\n",
    "COPY pyproject.toml .\n",
    "COPY uv.lock .  # Optional, if you have a lock file\n",
    "RUN uv sync\n",
    "\n",
    "# Copy the actual app code\n",
    "COPY . /app\n",
    "\n",
    "# Expose FastAPI port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Start the API server\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nDockerfile:\")\n",
    "print(dockerfile_content)\n",
    "print(\"Note: This Dockerfile assumes your FastAPI app is defined in a standalone Python file (e.g., `main.py`). If you're currently running everything in a Jupyter notebook, you'd need to extract the API logic into a file for deployment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Criteria\n",
    "- Implementation that can process reviews and return sentiments\n",
    "- Use of extra data to improve predictions\n",
    "- Proper API design\n",
    "- Reasonable deployment strategy\n",
    "\n",
    "Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
