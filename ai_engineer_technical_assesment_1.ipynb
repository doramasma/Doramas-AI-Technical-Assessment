{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Engineer Technical Assessment\n",
    "\n",
    "## Overview\n",
    "Build an AI-powered solution for sentiment analysis of movie reviews that leverages the existing dataset to improve accuracy. This assessment is designed to be completed in 2-3 hours, we do NOT expect very detailed answers or long explanations.\n",
    "\n",
    "## Notes\n",
    "- AI assistance is allowed and, in fact, encouraged. caveats are:\n",
    "    - Concise explanations and simple code are preferred\n",
    "    - Solutions that use newer information and go beyond LLMs cuttof date are valuable.\n",
    "    - You must be able to explain the code you write here\n",
    "\n",
    "- Look up any information you need, copy and paste code is allowed.\n",
    "- Setup the environment as needed. You can use your local environment, colab, or any other environment of your preferenc.\n",
    "- Focus on working solutions, leave iteration and improvements if you have extra time.\n",
    "\n",
    "## Setup\n",
    "The following cells will download and prepare the IMDB dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 5000\n",
      "Test samples: 10\n",
      "\n",
      "Sample review:\n",
      "Text: Dumb is as dumb does, in this thoroughly uninteresting, supposed black comedy. Essentially what starts out as Chris Klein trying to maintain a low profile, eventually morphs into an uninspired version...\n",
      "Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load IMDB dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "\n",
    "# Sample subset for quicker development\n",
    "train_df = train_df.sample(n=5000, random_state=42)\n",
    "test_df = test_df.sample(n=10, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample review:\")\n",
    "sample = train_df.iloc[0]\n",
    "print(f\"Text: {sample['text'][:200]}...\")\n",
    "print(f\"Sentiment: {'Positive' if sample['label'] == 1 else 'Negative'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6868</th>\n",
       "      <td>Dumb is as dumb does, in this thoroughly unint...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24016</th>\n",
       "      <td>I dug out from my garage some old musicals and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9668</th>\n",
       "      <td>After watching this movie I was honestly disap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13640</th>\n",
       "      <td>This movie was nominated for best picture but ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14018</th>\n",
       "      <td>Just like Al Gore shook us up with his painful...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "6868   Dumb is as dumb does, in this thoroughly unint...      0\n",
       "24016  I dug out from my garage some old musicals and...      1\n",
       "9668   After watching this movie I was honestly disap...      0\n",
       "13640  This movie was nominated for best picture but ...      1\n",
       "14018  Just like Al Gore shook us up with his painful...      1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Model Implementation\n",
    "Implement a solution that analyzes sentiment in movie reviews. This part is explicitly open-ended: Explore ways to leverage the example dataset to enhance predictions. You can consider a pre-trained language model that can understand and generate text, external API's, RAG systems etc. \n",
    "Feel free to use any library or tool you are comfortable with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address this sentiment analysis assessment, I have chosen to evaluate two approaches:\n",
    "\n",
    "1. **Lightweight Encoder Model (ModernBERT):** Using an encoder-only transformer model trained for sentiment classification. Specifically, I have decided to use [ModernBERT](https://arxiv.org/pdf/2412.13663), a next-generation encoder model (2024) that introduces several architectural advancements over the original BERT.\n",
    "2. **Sentiment Classification with a Small LLM:** The second approach explores the use of a small-scale Large Language Model (LLM), ranging between 1B to 3B parameters, to perform sentiment classification through prompt engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightweight Encoder Model (ModernBERT)\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "class LightweightModelService:\n",
    "    def __init__(self, model_name: str =\"clapAI/modernBERT-base-multilingual-sentiment\") -> None:\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Load the tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, torch_dtype=torch.float16).to(self.device).eval()\n",
    "\n",
    "        print(f\"Retrieve labels from the model's configuration  {self.model.config.id2label}\")\n",
    "\n",
    "    def predict(self, movie_review: str) -> tuple[str, float]:\n",
    "        inputs = self.tokenizer(movie_review, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = self.model(**inputs)\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            confidence, prediction = torch.max(probs, dim=-1)\n",
    "\n",
    "        label_map = self.model.config.id2label\n",
    "        sentiment = label_map[prediction.item()].lower()  \n",
    "\n",
    "        # Map neutral to negative for binary compatibility\n",
    "        if sentiment == \"neutral\":\n",
    "            sentiment = \"negative\"\n",
    "\n",
    "        return sentiment, round(confidence.item(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Any, cast\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline  # type: ignore\n",
    "\n",
    "### Instructions:\n",
    "# System-level prompt defining the assistant's behavior\n",
    "def get_system_prompt() -> str:\n",
    "    return \"\"\"You are an expert in sentiment analysis of movie reviews. Your task is to evaluate the sentiment of the given review and classify it as \"positive\" or \"negative\", and provide a confidence between 0-1. Your analysis must consider both explicit and implicit sentiment cues while maintaining a focus on the target entity. \n",
    "    \n",
    "    1. **Sentiment Classification Criteria**:  \n",
    "    - **Positive**: The review expresses favorable opinions about the movie, highlighting its strengths or praising specific aspects.  \n",
    "    - **Negative**: The review conveys unfavorable opinions, criticizing elements of the movie or expressing disappointment.\n",
    "\n",
    "    3. **Handling Mixed Sentiments and Implicit Sentiment**:  \n",
    "    - If both positive and negative elements exist, classify based on the **dominant sentiment**, considering intensity and frequency.  \n",
    "    - Detect subtle tones, including **irony, implied sentiment, and framing biases** (e.g., selective comparisons, loaded phrases).  \n",
    "\n",
    "    5. **Output Format**:  \n",
    "    - Return:\n",
    "        classification: <positive|negative>  \n",
    "        confidence: <float between 0 and 1>\n",
    "    - Do not include explanations, additional text, or punctuation.\n",
    "    \"\"\"\n",
    "\n",
    "# User-level prompt defining the interaction model\n",
    "def get_user_prompt(movie_review: str) -> str:\n",
    "    return f\"\"\"Analyze the sentiment of the provided movie review: {movie_review}\"\"\"\n",
    "\n",
    "\n",
    "class Phi4LLM:\n",
    "    \"\"\"\n",
    "    A service class for interacting with the PHI 4 model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model_name: str = \"microsoft/Phi-4-mini-instruct\", temperature: float = 0.1, max_new_tokens: int = 500\n",
    "    ) -> None:\n",
    "        torch.random.manual_seed(0)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.temperature = temperature\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.initialize_model()\n",
    "\n",
    "    def initialize_model(self) -> None:\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            **self.get_model_kwargs(),\n",
    "        ).eval()\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.completion_pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.model,\n",
    "                tokenizer=self.tokenizer,\n",
    "            )\n",
    "\n",
    "    def get_completions(self, movie_review: str) -> tuple[str, float]:\n",
    "        \"\"\"\n",
    "        Generate completions based on the provided messages.\n",
    "        \"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": get_system_prompt()},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": get_user_prompt(movie_review),\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        generation_args = {\n",
    "            \"max_new_tokens\": self.max_new_tokens,\n",
    "            \"return_full_text\": False,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"do_sample\": False,\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = self.completion_pipeline(messages, **generation_args)\n",
    "\n",
    "        return self.parse_llm_output(cast(str, output[0][\"generated_text\"]))\n",
    "    \n",
    "    def parse_llm_output(self, llm_output: str) -> tuple[str, float]:\n",
    "        \"\"\"\n",
    "        Parse the LLM response to extract sentiment and confidence.\n",
    "        \"\"\"\n",
    "        classification_match = re.search(r\"classification:\\s*(positive|negative)\", llm_output, re.IGNORECASE)\n",
    "        confidence_match = re.search(r\"confidence:\\s*([0-1](?:\\.\\d+)?)\", llm_output)\n",
    "\n",
    "        if classification_match and confidence_match:\n",
    "            classification = classification_match.group(1).lower()\n",
    "            confidence = float(confidence_match.group(1))\n",
    "            return classification, confidence\n",
    "\n",
    "        raise ValueError(f\"Could not parse output: {llm_output}\")\n",
    "\n",
    "    def get_model_kwargs(self) -> dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Return default kwargs for huggingface model loading.\n",
    "        \"\"\"\n",
    "        model_kwargs = {\n",
    "            \"device_map\": \"auto\",\n",
    "            \"torch_dtype\": \"auto\",\n",
    "            \"trust_remote_code\": True,\n",
    "        }\n",
    "        # if self.device.type == \"cuda\":\n",
    "        #     model_kwargs[\"attn_implementation\"] = \"flash_attention_2\"\n",
    "\n",
    "        return model_kwargs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: API Implementation\n",
    "Create a simple API using FastAPI that serves your solution. The API should accept a review text and return the sentiment analysis result.\n",
    "\n",
    "Expected format:\n",
    "```python\n",
    "# Request\n",
    "{\n",
    "    \"review_text\": \"This movie exceeded my expectations...\"\n",
    "}\n",
    "\n",
    "# Response\n",
    "{\n",
    "    \"sentiment\": \"positive\",\n",
    "    \"confidence\": 0.92,\n",
    "    \"similar_reviews\": [\n",
    "        {},\n",
    "        {}\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve labels from the model's configuration  {0: 'negative', 1: 'neutral', 2: 'positive'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539da62091b84b7c9f56ac208fc66c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [18800]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:63401 - \"GET /health HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doram\\AppData\\Local\\Temp\\ipykernel_18800\\2032029779.py:62: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timestamp=datetime.utcnow().isoformat()\n",
      "c:\\Users\\doram\\Desktop\\Machine_learning\\Doramas-AI-Technical-Assessment\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:63428 - \"POST /sentiment-classification-llm HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:63466 - \"POST /sentiment-classification-lightweight HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:63466 - \"POST /sentiment-classification-lightweight HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:63466 - \"POST /sentiment-classification-lightweight HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "import uvicorn\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from fastapi import FastAPI, HTTPException\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "# Your API implementation here\n",
    "class ClassificationInputRequest(BaseModel):\n",
    "    review_text: str\n",
    "\n",
    "class ClassificationResponse(BaseModel):\n",
    "    sentiment: str # Should be positive or negative\n",
    "    confidence: Optional[float] \n",
    "    similar_reviews: List[Dict]\n",
    "\n",
    "class HealthCheckResponse(BaseModel):\n",
    "    status: str\n",
    "    version: str\n",
    "    timestamp: str  \n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "lightweight_model = LightweightModelService()\n",
    "llm_model = Phi4LLM()\n",
    "\n",
    "@app.post(\"/sentiment-classification-lightweight\", response_model=ClassificationResponse)\n",
    "async def sentiment_predict_lightweight(request: ClassificationInputRequest):\n",
    "    try:\n",
    "        result = lightweight_model.predict(request.review_text)\n",
    "        sentiment, confidence = result\n",
    "\n",
    "        return ClassificationResponse(\n",
    "            sentiment=sentiment,\n",
    "            confidence=confidence,\n",
    "            similar_reviews=[]\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        raise HTTPException(detail=str(e), status_code=400)\n",
    "\n",
    "@app.post(\"/sentiment-classification-llm\", response_model=ClassificationResponse)\n",
    "async def sentiment_predict_llm(request: ClassificationInputRequest):\n",
    "    try:\n",
    "        result = llm_model.get_completions(request.review_text)\n",
    "        sentiment, confidence = result\n",
    "\n",
    "        return ClassificationResponse(\n",
    "            sentiment=sentiment,\n",
    "            confidence=confidence,\n",
    "            similar_reviews=[]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(detail=str(e), status_code=400)\n",
    "    \n",
    "@app.get(\"/health\", response_model=HealthCheckResponse)\n",
    "async def health_check():\n",
    "    return HealthCheckResponse(\n",
    "        status=\"healthy\",\n",
    "        version=\"1.0.0\",\n",
    "        timestamp=datetime.utcnow().isoformat()\n",
    "    )\n",
    "\n",
    "# Code for run a FastAPI server inside a Jupyter notebook\n",
    "# def run_api():\n",
    "#     uvicorn.run(app, host=\"0.0.0.0\", port=8000, reload=False)\n",
    "\n",
    "# thread = threading.Thread(target=run_api, daemon=True)\n",
    "# thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Testing and Performance\n",
    "Evaluate your solution's performance on the test set. Include:\n",
    "1. Accuracy metrics (precision, recall, F1-score)\n",
    "2. Inference speed (average time per prediction)\n",
    "\n",
    "Compare performance with and without using the example data to demonstrate any improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Your testing code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Deployment Strategy\n",
    "\n",
    "1. Describe your deployment strategy considering:\n",
    "   - Data storage and retrieval\n",
    "   - Scalability\n",
    "   - Resource requirements\n",
    "   - Cost considerations\n",
    "\n",
    "2. Create a simple Dockerfile to package your solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Deployment Strategy\n",
      "\n",
      "## Infrastructure\n",
      "...\n",
      "\n",
      "## Scalability Approach\n",
      "...\n",
      "\n",
      "## Model & Data Storage\n",
      "...\n",
      "\n",
      "## Resource & Cost Considerations\n",
      "...\n",
      "\n",
      "\n",
      "Dockerfile:\n",
      "\n",
      "# Your Dockerfile here\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your deployment strategy here as a markdown cell\n",
    "deployment_strategy = \"\"\"\n",
    "# Deployment Strategy\n",
    "\n",
    "## Infrastructure\n",
    "...\n",
    "\n",
    "## Scalability Approach\n",
    "...\n",
    "\n",
    "## Model & Data Storage\n",
    "...\n",
    "\n",
    "## Resource & Cost Considerations\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "print(deployment_strategy)\n",
    "\n",
    "# Write your Dockerfile content\n",
    "dockerfile_content = \"\"\"\n",
    "# Your Dockerfile here\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nDockerfile:\")\n",
    "print(dockerfile_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Criteria\n",
    "- Implementation that can process reviews and return sentiments\n",
    "- Use of extra data to improve predictions\n",
    "- Proper API design\n",
    "- Reasonable deployment strategy\n",
    "\n",
    "Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
